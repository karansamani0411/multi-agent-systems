{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789242a7-eb6f-44e1-99dd-bda8849835f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install necessary libraries\n",
    "\n",
    "! pip install -qU langchain groq langchain-groq langchain_community duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da31f456-b869-40d8-88b8-4252aac4e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Groq API key:  ········\n"
     ]
    }
   ],
   "source": [
    "## API Key\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "#gets api key from the notebook and stores it in the current environment\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1010d5-471e-4e84-a4d3-c5a595b09d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "#import for agentic llm\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50ecdefa-9e99-4d05-b18f-73961da17dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use llama3 model from Groq Cloud\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5325f74-c9aa-41a6-b6ff-a82c60db4aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize search tool\n",
    "search = DuckDuckGoSearchRun()\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fa7abb9-e547-4a6c-b7b5-31e37a21bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intializing agent with internet search tool\n",
    "agent = initialize_agent(tools, llm, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3638fb80-bcc3-4236-85e9-101b43232da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should search for recent news about Meta's large language models to get the latest information.\n",
      "Action: duckduckgo_search\n",
      "Action Input: \"Meta large language models latest news\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMeta is committed to openly accessible AI. Read Mark Zuckerberg's letter detailing why open source is good for developers, good for Meta, and good for the world.; Bringing open intelligence to all, our latest models expand context length to 128K, add support across eight languages, and include Llama 3.1 405B—the first frontier-level open source AI model. Today, we're introducing Meta Llama 3, the next generation of our state-of-the-art open source large language model. Llama 3 models will soon be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from hardware platforms offered by AMD, AWS, Dell, Intel, NVIDIA, and Qualcomm. The release comes as tech companies are racing to show that their growing portfolios of resource-hungry large language models can ... Meta's model posted a score of 73.8, compared to GPT-4o's 76.6 ... Large Language Model. The Llama Ecosystem: Past, Present, and Future. September 27, 2023•. 3 minute read. It's been roughly seven months since we released Llama 1 and only a few months since Llama 2 was introduced, followed by the release of Code Llama. In short, the response from the community has been staggering. We're releasing Llama 3.1 405B, the first frontier-level open source AI model, as well as new and improved Llama 3.1 70B and 8B models. In addition to having significantly better cost/performance relative to closed models, the fact that the 405B model is open will make it the best choice for fine-tuning and distilling smaller models.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know that Meta has released its latest large language model, Llama 3, and it's an open-source model that expands context length to 128K, supports eight languages, and includes Llama 3.1 405B, the first frontier-level open source AI model.\n",
      "\n",
      "Final Answer: Meta has released its latest large language model, Llama 3, which is an open-source model that expands context length to 128K, supports eight languages, and includes Llama 3.1 405B, the first frontier-level open source AI model.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Meta has released its latest large language model, Llama 3, which is an open-source model that expands context length to 128K, supports eight languages, and includes Llama 3.1 405B, the first frontier-level open source AI model.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the latest news about Meta's large language models ?\"\n",
    "result = agent.run(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2300c04a-a8ef-4edc-8862-a74276ed920d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
