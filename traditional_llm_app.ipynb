{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11856fb8-c7fb-4e75-b80b-89f9947681f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## install packages\n",
    "\n",
    "!pip install -qU langchain groq langchain_community langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c021846a-56a1-4b3d-b732-716fba0f8680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Groq API key:  ········\n"
     ]
    }
   ],
   "source": [
    "## API Key\n",
    "\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "#gets api key from the notebook and stores it in the current environment\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59b228f6-b8d1-4db7-bd94-0fc12d467d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary imports\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#use llama3 model from Groq Cloud\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0518b3e5-6713-40bd-a06f-5e93bca0a6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful and knowledgeable assistant that answers questions clearly and concisely.'), HumanMessage(content='Explain a transformer model in detail')])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create prompt tempate for chat\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful and knowledgeable assistant that answers questions clearly and concisely.\"),\n",
    "        (\"human\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "prompt.invoke({\"question\": \"Explain a transformer model in detail\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b860b0a-50f0-4ebf-b11c-78a6604e93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a chain which takes llm and correspinding prompt as an input\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "response = chain.invoke({\"question\" : \"What is a tranformer model?\"}) #invoke llm chain with a user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81bbc66f-6b35-4065-9e5f-b6cf7a22dc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A transformer model is a type of neural network architecture primarily used for natural language processing (NLP) tasks, such as machine translation, text summarization, and language modeling. It was introduced in a research paper by Vaswani et al. in 2017 and has since become a widely used and popular architecture in the field of NLP.\n",
      "\n",
      "The transformer model is designed to handle sequential data, such as text, by using self-attention mechanisms to weigh the importance of different input elements relative to each other. This allows the model to focus on specific parts of the input sequence and capture long-range dependencies and relationships.\n",
      "\n",
      "The key components of a transformer model are:\n",
      "\n",
      "1. Encoder: This is the input side of the model, which takes in a sequence of tokens (such as words or characters) and outputs a continuous representation of the input sequence.\n",
      "2. Decoder: This is the output side of the model, which generates the output sequence one token at a time, based on the input sequence and the previous tokens generated.\n",
      "3. Self-Attention Mechanism: This is the core component of the transformer model, which allows the model to attend to different parts of the input sequence and weigh their importance relative to each other.\n",
      "4. Feed-Forward Network (FFN): This is a fully connected neural network that is used to transform the output of the self-attention mechanism.\n",
      "\n",
      "The transformer model has several advantages over traditional recurrent neural network (RNN) architectures, including:\n",
      "\n",
      "* Parallelization: The transformer model can be parallelized more easily than RNNs, making it faster and more efficient to train.\n",
      "* Scalability: The transformer model can handle longer input sequences and larger models than RNNs.\n",
      "* Flexibility: The transformer model can be used for a wide range of NLP tasks, including machine translation, text summarization, and language modeling.\n",
      "\n",
      "Overall, the transformer model has revolutionized the field of NLP and has been widely adopted in many applications, including Google's BERT and T5 language models.\n"
     ]
    }
   ],
   "source": [
    "#print responses\n",
    "print(response['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fa24d-cb0b-4c29-864a-4af8f670ee1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
